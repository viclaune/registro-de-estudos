Vamos utilizar o EMR, uma ferramenta para Big Data da AWS. Vamos fazer uma configuração de um cluster EMR, configurar notebooks para testes com Spark e executar jobs Spark via *spark-submit*.

A ferramenta mais utilizada em Engenharia de Dados é o Spark. Para trabalhar com o Spark na AWS utilizamos o EMR (Elastic MapReduce).

1. No Search vamos buscar EMR. E Vamos criar um cluster EMR, basicamente é um cluster hadoop, spark para conectar notebooks e fazer testes nele ou para criar um cluster spark para enviar jobs. Em Create Cluster - Advanced Options → Vamos selecionar no Releasa o Spark mais atual e vamos adicionar: Hadoop 3.2.1, JupyterHub 1.2.0, Hive 3.1.2, Jupyter EnterpriseGateway 2.1.0, Hue 4.9.0, Spark 3.1.1., Livy 0.70, Pig 0.17.0. Não vamos marcar nada de em **Multiple master nodes** e nem em  **AWS Glue Data Catalog settings (optional)**. **Steps (optional):** vamos colocar o concurrency → 10. Em Hardware não vamos definir nada, e a Cluster Composition vamos deixar uniform instance groups. Network: redes default. Cluster Nodes and Instances: obrigatoriamente precisamos ter um master e alguns cores para rodar, instância de task não são necessária, podemos escolher qual a máquina, por default vem a m5.xlarge, mas em ambiente de produção para cluster spark, o melhor são a da família r, pois possui mais memória. Outra coisa legal, são as instâncias On-demand ou Spot, o Sport são muito mais baratas que a On-demand, a diferença é que a AWS pode requisitar a máquina quando necessário e dar uma pausa de 1-2 min. Não vamos habilitar o escalonamento em *Cluster* *****Scaling*, e o *EBS Root Volume* pode ficar default. **General** **Options**: colocamos o nome do cluster spark e não queremos proteção de termination. Se quisermos adicionar alguma biblioteca python, precisamos fazer a adição no *Bootstrap Actions*. Em Security Options: Colocamos No key pairs found, e deixamos tudo default. E **Create Cluster**.

2. Vamos agora fazer a **criação dos notebooks**. Create notebook → Colocar nome, descrição, escolher o cluster, security group podemos deixar default; podemos selecionar se queremos guardar no cluster ou em um S3. Se eu quiser trabalhar com algum código em repositório git podemos linkar também.

3. No S3 foi carregado os dados do ENEM 2019;  No EMR on EC2, na aba Notebooks podemos abrir o JupyterLab ou o Jupyter Notebook para utilizar o Spark (prefira utilizar o JupyterLab). **Configurar o kernel:** PySpark. **O que vamos fazer?** Vamos abrir os dados do S3 direto no EMR com Spark.
    
    ```python
    import pyspark
    
    # Ler os dados do enem 2019
    enem = (
    	spark
    	.read
    	.format("csv")
    	.option("header", True)
    	.option("inferSchema", True)
    	.option("delimiter", ";")
    	.load(<URI>)
    )
    
    enem.printSchema()
    
    df = (
    			enem
    			.select("NU_IDADE", "TP_SEXO", "TP_ESTADO_CIVIL", "TP_NACIONALIDADE"
    							"NU_NOTA_CN", "NU_NOTA_CH", "NU_NOTA_LC", "NU_NOTA_MT")
    )
    
    df.count()
    
    from pyspark.sql.functions import mean, max, min, col
    
    (
    	df
    	.groupby("TP_SEXO")
    	.agg(
    			 mean(col("NU_IDADE")).alias("MED_IDADE"),
    			 count(col("TP_SEXO")).alias("CONTAGEM"),
           max(col("NU_NOTA_MT")).alias("MAX_NOTA_MT"),
    			 min(col("NU_NOTA_MT")).alias("MIN_NOTA_MT"),
    			
    	)
    	.show()
    )
    
    # Vamos agora fazer a conversão em parquet
    (
    	enem
    	.write()
    	.mode("overwrite")
    	.format("parquet")
    	.partitionBy("year")
    	.save("stagin/enem")
    )
    
    ```
    
4. Vamos executar a operação em um arquivo `.py` ao invés de fazer no kernel do PySpark.
    
    ```python
    from pyspark.sql.functions import mean, max, min, col, count
    from pyspark.sql import SparkSession
    
    spark = (
    	SparkSession.builder.appName("ExerciseSpark")
    	.getOrCreate()
    )
    
    enem = (
    	spark
    	.read
    	.format("csv")
    	.option("header", True)
    	.option("inferSchema", True)
    	.option("delimiter", ";")
    	.load("s3://nome-do-datalake/nome-da-pasta/enem")
    )
    
    (
    	enem
    	.write()
    	.mode("overwrite")
    	.format("parquet")
    	.partitionBy("year")
    	.save("s3://nome-do-datalake/nome-da-pasta/stagin/enem")
    )
    ```
    
    Vamos salvar esse arquivo `.py` localmente, no S3 criaremos um nova folder chamada EMR-Code, e vamos criar outra folder dentro dessa com o nome pyspark para definir que são códigos PySpark, é uma boa prática manter tudo bem organizado e fazemos o upload do arquivo `.py` local.
    
5. Stopar o spark, o notebook. E no cluster Spark vamos em Steps e adicionamos um step para executar um código spark, no JAR location passamos `commando-runner.jar`, e nos argumentos passamos o `spark-submit --master yarn --deploy mode cluster <URI-do-código-.py-carregado-no-S3>`.

6. Para validar que tudo está certinho, vamos pegar o novo dado escrito (parquet) e verificar se está igual o arquivo `.csv`
    
    ```python
    import pyspark
    
    # Lendo os arquivos em modo parquet
    enem_parquet = (
    		spark
    		.read
    		.format("parquet")
    		.load("s3://nome-do-data-lake/staging/enem")
    )
    
    enem_parquet.printSchema
    enem_parquet.show()
    ```
    
7. Não esqueça de sempre fazer o Terminate do cluster spark para não ter surpresas no cartão de crédito. Na prática, os 56 minutos de uso custou 8 centavos de dólares, 4 centavos para cada core usado.