Vamos nos aprofundar no serviço S3 da AWS, interagir com o bucket pelo console da AWS e interagir usando o SDK oficial da AWS, o boto3.

1. Na página do S3 teremos os buckets já existentes. Percebam que no S3 não temos uma região específica, pois S3 não requer uma seleção de região. Os buckets são atribuídos à uma região por completo, tem replicação em todas as zonas de disponibilidade, tem backup automatizado, entre outras coisas.
2. Amazon S3 → Create bucket → **General configuration:** Em bucket name os nomes precisam ser globalmente únicos, não pode ser o nome de um bucket que já existe e nem que já existiu. Na AWS Region selecionamos apenas a região, mas não as zonas. **Block Public Access settings for this bucket:** Por default, o acesso público é bloqueado, sendo o acesso exclusivo por controle de usuários. **Bucket Versioning:** Interessante quando é preciso voltar à uma versão antiga de um projeto, desabilitar para não gerar custos. **Tags:** Excelente prática tagear os recursos. 
3. Trabalhar com o Data Lake é extremamente simples, podemos criar uma nova pasta (**Create** **New** **Folder**). Para subir um dado para o Data Lake podemos fazê-lo através do browser, ou até arrastando e colando na página. Não é possível visualizar os dados no browser, apenas os metadados. Mas vamos fazer o upload de um arquivo `.csv` qualquer para trabalhar com ele posteriormente.
4. Normalmente trabalhamos com o S3 através de código. Podemos interagir com o AWS através da linha de comando utilizando o `aws cli` (é preciso instalar antes). Quando vamos acessar de maneira remota a AWS, precisamos configurar as **credenciais de acesso**, para isso temos que buscar o serviço do **IAM**, buscar o nosso usuário, e teremos na nossa Security Credentials as chaves de acesso programático (Key ID e Access Key Id) de posse das duas classes, vamos configurar no terminal.
    
    ```bash
    aws --version 
    
    aws configure
    AWS Access Key ID [***************aaa]: <colocar_o_codigo>
    AWS Secret Access Key [****************aaaaa: <colocar_o_codigo>
    Default region name [us-east-1]: <digitar_a_region>
    Default output format [json]: json
    ```
    
5. Vamos interagir agora com o s3 a partir do código Python. Criando um arquivo `interact_s3.py`. A biblioteca Python para interação com AWS oficial é o `boto3`. Vamos importar também o `pandas` para visualizar um dado `.csv` que foi subido no bucket. **Criando um cliente para interagir com o S3**: Vamos chamar o cliente de `s3_client`, vamos passar o `'s3’` como parâmetro para informar que é um cliente S3. **Download do objeto no S3**: Utilizamos o comando `.download_file()`, que receberá 3 argumentos, o primeiro é o nome do bucket que está salvo, o segundo é o nome do objeto, o terceiro é o nome da pasta local que receberá o objeto. **Fazendo um Upload via código para o S3**: Utilizamos o comando `upload_file()`, que receberá 3 argumentos, o primeiro o path local, o segundo é o nome do bucket, e o terceiro é como o objeto irá se chamar no bucket.
    
    ```python
    import boto3
    import pandas as pd
    
    # Criando um cliente para interagir com o AWS S3
    s3_client = boto3.client('s3')
    
    s3_client.download_file("<nome_do_bucket>", 
    										    "data/<nome_do_objeto_no_bucket>", 
                            "data/<nome_do_objeto_local>")
    
    df = pd.read_csv(<path>, sep=";")
    print(df)
    
    s3_client.upload_file("data/<nome_do_objeto_local",
                          "<nome_do_bucket>",
    											"data/nome_do_objeto_no_bucket")
    
    ```
    
6. **Diferenças entre subir pelo `boto3`, pelo `cli` ou pelo console**: No console temos um limite do tamanho do arquivo, alguns gigas já começam a pesar e se torna um pouco difícil. Via `boto3` e `cli` a  gente consegue subir arquivos grandes, o limite do S3 por arquivo é 5 TB.