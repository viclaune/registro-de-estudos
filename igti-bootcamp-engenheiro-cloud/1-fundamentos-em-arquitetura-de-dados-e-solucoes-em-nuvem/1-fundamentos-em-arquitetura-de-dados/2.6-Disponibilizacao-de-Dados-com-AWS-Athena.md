Vamos aprender a configurar e configurar Glue Crawlers para construir o nosso catálogo de dados, vamos configurar o nosso AWS Athena e fazer consultas SQL no nosso Data Lake.

1. Com os dados no Data Lake, vamos disponibilizar os dados para consumo. Uma forma fácil de fazer isso é com o DW, na AWS temos o serviço Redshift que é um serviço de DW muito eficiente, mas não vamos usá-lo, apesar de eficiente é também muito caro. E outras alternativas se tornam viáveis, para isso, vamos utilizar uma Engine de Data Lake, o AWS Athena. Com esse serviço, podemos fazer consultas diretamente aos arquivos do Lake (que estão no S3), para isso, vamos precisar utilizar o Glue, para criar um Glue Crawler para entender o esquema das tabelas que estão no Lake para que o Athena possa fazer consultas SQL simples.

2. Glue → Crawlers. Vamos configurar crawlers para entender o esquema de tabelas que estão no nosso lake, vamos copiar o URI da pasta em que estão os arquivos. Vamos dar um nome ao crawler, vamos colocar nossas TAGs → Next → **Specify crawler source type**: Selecionar os data stores e para crawlear as pastas. → Next → Add a data store: selecionamos o tipo de store, no caso S3, indicamos o path da pasta que será feita o crawler. → Vamos ter que criar um IAM role específico para isso → Next → Devemos determinar a
frequência, e devemos colocar que vamos executar on demand. Se a tabela não for atualizada, não precisamos rodar mais de uma vez. **Configure the crawler’s output**: Não temos nenhuma database do Athena nessa região, então vamos adicionar um database, damos um nome (‘datalake’ no caso), sendo importante dar um nome a partir da zona do Data Lake (obs.3). → Next (default) → Resumo do Crawler → Finish!

3. Agora vamos rodar o Crawler. E voltamos no Athena e damos um Get Started. Vamos usar a interface do Athena para executar querys SQL diretamente contra os arquivos do datalake (acabado de criar). Antes de executar a primeira query, é preciso definir onde o Athena irá guardar o resultado das consultas, vamos então em Settings e definimos em que pasta do Data Lake ele será guardado. (Interessante criar uma pasta para definir).

```sql
select * from pnade20203
limit 10;
```

4. Temos os dados da tabela em arquivo parquet, fazendo a leitura do arquivo diretamente no Athena, sem database, sem instância de computação, sem nada. Ele consegue entender os metadados exportados de cada tabela a partir do crawler e faz consultas diretamente nesses arquivos, ele mostra qual foi o volume e o tempo gasto para executar a consulta, você vai pagar pela volume de dados varridos nas consultas. Não vamos pagar pela database parada, vamos usar pelo seu uso. Para dados com volumes razoáveis, o uso é tranquilo, inclusive podemos executar querys mais complexas.

```sql
select 
	sexo
	count(sexo) as contagem
	avg(idade) as med_idade
	min(idade) as min_idade
	max(idade) as max_idade
from pnade20203
group by sexo;
```

Obs.: Sempre que for ingerir um dado dentro da estrutura do Data Lake, ele deve ir em raw-data.
Obs.2: Nunca apontamos o crawler para um arquivo, sempre para um pasta. Geralmente as pastas possuem vários arquivos e trabalham de forma particionada, então no crawler, sempre se entende como pastas.
Obs.3: Fazemos uma database para dados raw, staging… para que as pessoas saibam qual em qual zona estão os dados, pois dados em zonas raw e staging não estão otimizadas para consulta. E pode varrer muito dado e a consulta ficar mais cara.